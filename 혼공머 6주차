# 07-1

# 패션 MNIST

교재 설정 : 패션MNIST 데이터를 텐서플로를 사용해 불러옴.

![image.png](attachment:21a227b1-a6d4-4e37-acc2-007f15adc43a:image.png)

→ 휸련 데이터는 60,000 개의 이미지로 구성, 사이즈는 28*28, 타깃도 60,000 개의 원소가 있는 1차원배열

→ 테스트 세트는 10,000 개의 이미지로 구성

 

![image.png](attachment:f079438e-c37d-48ff-831f-3d70bc9b62e6:image.png)

훈련 데이터에서 몇 개의 샘플을 그림으로 출력해봤을 때 요런 종류의 옷들(데이터들) 이 있음

![image.png](attachment:3fa59f93-afc6-4a43-a472-57e0663004e8:image.png)

처음 10개 샘플의 타깃값을 리스트로 만든 후 출력해봤을 때 저렇게 나옴.

패션 MNIST의 타깃은 0~9까지의 숫자 레이블로 구성됨. 아직 각 숫자가 뭘 의미하는지는 모르겠지만 마지막 2개의 샘플이 같은 레이블(숫자 5)를 가지고 있음. 패션 MNIST에 포함된 10개의 레이블의 의미는 다음과 같다

| 레이블 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 패션아이템 | 티셔츠 | 바지 | 스웨터 | 드레스 | 코트 | 샌달 | 셔츠 | 스니커즈 | 가방 | 앵클 부츠 |
|  |  |  |  |  |  |  |  |  |  |  |

지금 각 레이블바다 정확히 6,000 개의 샘플이 들어있는 상태

# 로지스틱 회귀로 패션아이템 분류하기

60,000 개나 되는 데이터를 한꺼번에 사용하여 모델을 훈련하는 것보다 샘플을 하나씩 꺼내서 모델을 훈련하는 방법이 더 효율적을 보임 → 확률적 경사하강법이 적합하다고 보인다

*확률적 경사하강법(SGD) : 그냥 GD 는 매 학습마다 모든 데이터를 사용했다면 SGD 는 한 학습당 한 개의 데이터를 임의 추출해서 사용

![image.png](attachment:f2f04184-bd9c-4be1-9ac0-d35dab7e523f:image.png)

→ SGD 는 2차원 입력을 다루지 못하므로 샘플을 1차원으로 만들어줌

→ 반복 횟수를 우선 5번으로 지정하고 했지만, 반복 횟수를 여기서 늘린다고 해도 성능이 딱히 크게 향상되지는 않음

로지스틱 회귀 공식

![image.png](attachment:14c223f2-3845-4df3-874c-731640eb3082:image.png)

이 식을 패션 MNIST 데이터에 맞게 변형하면

![image.png](attachment:f06ff1ab-2729-4428-8532-d959455a2798:image.png)

w1,w2.. 이것들은 가중치임

![image.png](attachment:629f9083-5b58-455b-b2bf-6d121368d69c:image.png)

이건 바지 레이블에 대한 방정식인데 티셔츠 식이라 매우 유사. 하지만 바지에 대한 출력을 계산하기 위해 가중치와 절편은 다른 값을 사용해야함. 티셔츠와 바지와 같은 가중치를 사용한다면 구분할 수 없음

나머지 클래스에 대해서도 이런 식으로 선형방정식을 가진다고 볼 수 있음.

SGDclassifier 모델은 패션 MNIST 데이터의 클래스를 가능한 잘 구분할 수 있도록 10개의 방적시에 대한 모델 파라미터(가중치와 절편) 을 찾음

![image.png](attachment:0e3cb6a1-ef10-4f20-9fad-9d9a43dc0be7:image.png)

그림 해석

→ 첫 번째 픽셀1이 w1과 곱해쳐서 z_티셔츠에 더해짐

→ 두 번째 픽셀2도 w2와 곱해져서 z_티셔츠에 더해짐

→ 마지막 필셀784도 w784와 곱해져서 z_티쳐스에 더하고 절편 b를 더함

→ 다른 레이블(바지, 구두 등)에 대해서도 동일하게 진행

*중요한 점은 레이블마다 픽셀에 곱해지는 가중치와 더해지는 절편이 달라야한다는 점

# 인공 신경망

가장 기본적인 인공 신경망은 확률적 경사 하강법을 사용하는 로지스틱 회귀와 같음 

![image.png](attachment:a585cafb-1e3c-4305-804c-0d2797e862af:image.png)

아까 티셔츠와 바지에 대한 로지스틱 회귀식을 그림으로 표현한 것과 매우 유사

z1~z10 은 신경망의 최종값으로써 출력층(output layer)

z 값을 계산하는 단위를 뉴런(=유닛)

x1~x784 는 입력층(input layer) : 입력층은 픽셀값 자체이고 계산을 수행하지 않음

확률적 경사 하강법을 사용한 로지스틱 회귀 모델이 가장 간단한 인공 신경망이라면 성능이 일단 제일 좋을 것 같지는 않음. 이제 딥로닝 라이브러리인 텐서플로를 사용해 인공신경망을 만들어 보겠음

# 인공 신경망으로 모델 만들기

로지스틱 회귀에서 만든 훈련 데이터를 사용하겠음

로지스틱 회귀에서는 교차 검증을 사용해 모델을 평가했지만, 인공 신경망에서는 교차 검증을 잘 사용하지 않고 검증 세트를 별도로 덜어내어 사용

이렇게 하는 이유는

1. 딥러닝 분야의 데이터셋은 충분히 크기 때문에 검증 점수가 안정적
2. 교차 검증을 수행하기에는 훈련 시간이 너무 오래 걸림

![image.png](attachment:803e0562-8b70-4eef-baca-a984acf248ca:image.png)

가장 기본이 되는 층은 밀집층(dense layer) : 왼쪽에 있는 784개의 픽셀과 오른쪽에 있는 10개의 뉴런이 모두 연결된 걸 생각해보면 784*10 = 7840 개의 연결된 선이 있음. 아무튼 많아서 밀집층.

![image.png](attachment:78bba8df-5386-46cc-a8f4-b39d32e50feb:image.png)

이런 층을 양쪽의 뉴런이 모두 연결하고 있기 때문에 완전 연결층(fully connected layer) 이라고도 부름

케라스의 Dense 클래스를 사용해 밀집층을 만들어 보면

![image.png](attachment:ce74e09d-ff31-427e-8bde-020c2fd0b918:image.png)

첫번째 매개변수로 뉴런 개수를 10개로 지정

10개의 뉴런에서 출력되는 값을 확률로 바꾸기 위해서는 소프트맥스 함수를 사용(다중 분류니까)

(만약 2개의 클래스를 분류하는 이진 분류이면 시그모이드 함수를 사용하기 위해 activation 을 수정)

이후 sequential 클래스의 객체를 만들 때 앞에서 만든 밀집층의 객체 dense를 전달하고, 여기서 만든 model 객체가 바로 신경망 모델임

![image.png](attachment:c70cdeab-9bf5-4ae3-818c-6c69b9440b3d:image.png)

이진 분류의 출력 뉴런은 오직 양성클래스에 대한 확률(a)만 출력하기 때문에 음성 클래스에 관해서는 확률을 1-a 로 구할 수 있음.(이진 분류의 타깃값은 양성은 1, 음성은 0 으로 되있기 때문)

그럼, 다중분류에 대해서는?

![image.png](attachment:712ad03f-33ae-4a0b-b116-20f1c83407a0:image.png)

→ 출력층은 10개의 뉴런이 있고 10개의 클래스에 대한 확률을 출력함. 첫 번째 뉴런은 티셔츠일 확률, 두 번째 뉴런은 바지일 확률. 각 클래스에 대한 확률이 모두 출력되기 때문에 타깃에 해당하는 확률만 남겨 놓고 나머지 확률에는 0을 곱함

if. 샘플이 티셔츠일 겨우 [1,0,0,0,0,0,0,0] 이렇게 만들 수 있음

이 배열과 출력층의 활성화 값의 배열과 곱하면 됨.

![image.png](attachment:8bad71f6-85a0-4620-a94a-0869d6f5e6cd:image.png)

계산하고 나면 다른 원소는 0이 되고 a1 만 남음

신경망은 티셔츠 샘플에서 손실을 낮추려면 첫 번째 뉴런의 활성화 출력 a1의 값을 가능한 1에 가깝게 만들어야함

(타깃값을 해당 클래스만 1이고 나머지는 모두 0인 배열로 만드는 것을 원-핫 인코딩 이라고 함)

케라스 모델은 훈련할 때 기본으로 에포크 마다 손실값을 출력해줌. 정확도도 함께 출력해서 훈련이 잘되는지 확인하기 위해서 metrics 매개변수에 정확도 지표를 의미하는 accuracy 를 지정

*에포크 : 훈련 데이터 전체를 한 번 모델에게 모두 보여주는 것 (= 데이터를 처음부터 끝까지 1회 학습하는것, if 데이터가 100개면, 1에포크 = 100개를 전부 모델에 넣어 학습하는것)

*에포크 손실 : 한 에포크 동안 발생한 모든 손실(loss)를 평균한 값. 학습 데이터 전체를 1번 다 돌렸을 때 그 과정에서 모델이 얼마나 틀렸는지를 평균 낸 값

![image.png](attachment:09b91821-e6ba-448d-ab51-db1276424de2:image.png)

5번 반복하니까 정확도가 85% 를 넘었음. 검증 세트에서 모델 성능을 확인해 보면

![image.png](attachment:3361fe94-4c94-4b27-afbe-16b3e38edb44:image.png)

위에거랑 비슷함.(검증셋이 훈련 셋 보다 점수가 좀 낮은게 일반적) 83% 정도의 정확도

# 07-2

# 2개의 층

![image.png](attachment:d6f019db-7509-4a7b-91ef-2b3dbfd0a127:image.png)

→ 입력층과 출력층 사이에 밀집층이 추가된 형태. 입력층과 출력층 사이에 있는 층들은 은닉층

→ 활성화 함수 : 신경망 층의 선형 방정식의 계산 값에 적용하는 함수(소프트맥스 함수도 마찬가지임)

→ 출력층에 적용하는 활성화 함수는 제한되어 있음 

- 이진 분류 - 시그모이드
- 다중 분류 - 소프트맥스 함수

→ 은닉층의 활성화 함수는 시그모이드 함수, 렐루 함수 등을 사용

활성화 함수를 왜 쓰나?

→ 활성화 함수는 신경망이 단순한 직선 모델이 아니라 복잡한 곡선 패턴을 학습하도록 도와주는 핵심 요소다. 활성화 함수가 없으면 신경망은 아무리 층을 많이 쌓아도 결국 하나의 선형 함수와 똑같아져서 XOR 같은 간단한 비선형 문제도 못 푼다. 따라서 비선형성을 만들어서 복잡한 문제를 풀 수 있게함

## 심층 신경망 만들기

![image.png](attachment:0cbae7c4-fcce-46ae-8916-c759f7a3ecb4:image.png)

인공 신경망의 성능은 이렇게 층을 추가하여 입력 데이터에 대해 연속적인 학습을 진행하는능력에서 나옴

 

## 렐루 함수

![image.png](attachment:5f5b87fd-fc50-41a2-8af2-da9a774c5342:image.png)

초창기 인공신경망의 은닉층에서 활성화 함수로 많이 사용된 것은 시그모이드 함수임

하지만 왼쪽/오른쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속히 대응 불가

층이 많은 신경망일수록 그 효과가 누적되어 학습이 더 어려움

→ 이를 개선한게 렐루 함수

![image.png](attachment:dae9270a-e538-4eae-b120-fcc019ace717:image.png)

입력이 양수일 경우 입력을 그냥 통과시키고, 음수일 경우 0으로 만듦

렐루 홤수는 max(0,z) 와 같이 쓸 수 있음 : z가 0보다 크면 z 출력, 아니면 0 출력

## 옵티마이저

케라스에서 제공하는 경사 하강법 알고리즘들

![image.png](attachment:d1079073-000f-43e8-ad3c-7c91a9deb36b:image.png)

기본 경사 하강법 옵티마이저는 모든 SGD 클래스에서 제공함. 

## 07-3

## 손실 곡선

![image.png](attachment:ba342338-13e8-4ab9-8825-e2be8af468e2:image.png)

history 객체에는 훈련 측정값이 담겨있는 history 딕셔너리가 들어있음. → 손실과 정확도를 포함함

케라스는 기본적으로 에포크마다 손실을 계산함

![image.png](attachment:805ac9d5-a430-42fe-9d3c-0de58f1b2a91:image.png)

에포크가 0~4까지 x축에 포현됨. y축은 계산된 손실값

![image.png](attachment:44a29d13-dd5c-467c-9267-2f4f2189223a:image.png)

그래프들을 보면 에포크마다 손실은 감소하고 정확도는 향상됨.

→ 그럼 에포크를 계속 늘려서 학습하는 훈련 방향이 맞지 않나? 손실이 계속 감소하니까

![image.png](attachment:3db4941d-9bd3-4d4f-bbce-609eae177f90:image.png)

에포크 횟수를 20으로 늘리니까 진짜 손실이 더 잘 감소함.

## 검증 손실

인공신겨망도 모두 일종의 경사하강법을 사용하기 때문에 에포크에 따른 과대적합, 과소적합을 파악하려면 훈련 셋 뿐만 아니라 검증 셋에 대한 점수도 필요함

![image.png](attachment:97b6d7b8-2b1e-4307-b5a9-7635f1b66a3d:image.png)

초기에는 검증 손실이 감소하다가 다섯번째 에포크만에 다시 상승, 근데 훈련 손실은 계속 감소하기 때문에 훈련셋에만 과대적합된 모델이 만들어진 것. 

→ 검증 손실이 상승하는 시점을 최대한 뒤로 늦추면 검증 셋에 대한 손실이 줄어들 뿐만 아니라 검증 셋에 대한 정확도도 증가할 것

일단 과대 적합을 막아보자 → 옵티마이저 하이퍼파라미터 조정

![image.png](attachment:19974a48-e986-48e5-84fb-9390ad9891e1:image.png)

Adam 옵티마이저를 적용해본 것. 과대 적합이 꽤 줄었음. 요동치긴 하지만 열 번 째 에포크까지 전반적인 감소 추세가 이어짐. → Adam 이 꽤 괜찮다는 의미

## 드롭아웃

훈련 과정에 층에 있는 일부 뉴런을 랜덤하게 꺼서(출력을 0으로 만든다는 것) 과대적합을 막음

![image.png](attachment:2dc1e45a-1e41-4e88-80d1-45d0f5b47ecd:image.png)

드롭아웃이 어떻게 과대적합을 막냐? → 이전 층의 일부 뉴런이 랜덤하게 꺼지면 특성 뉴런에 과대하게 의존하는 것을 줄일 수 있고 모든 입력에 대해 주의 깊게 반응함. 일부 뉴런이 없을 수 있다는 것을 감안하면 더 안정적인 예측을 만들어 낼 수 도 있음

→ 물론 훈련이 끝난 뒤 평가나 예측, 즉 실제 사용할때는 드롭아웃 빼야함. 모든 뉴런을 사용해야하니까

![image.png](attachment:17aa5b2e-b5e2-4a7b-9409-692efdc47d5e:image.png)

드롭아웃 하니까 과대적합이 더 완홛됨. 열번째 좀 넘은 에포크에서 감소가 멈추긴 하지만 크게 상승하지 않아서 ㄱㅊ

## 콜백

훈련과정 중간 어떤 작업을 수행할 수 있게 해주는 객체

과대적합이 시작되기 전에 훈련을 미리 중지하는 조기종료(딥러닝 분야에서 널리 사용)

조기 종료는 훈련 에포크 횟수를 제한하는 역할이지만 과적합을 막아주기 때문에 규제 방법으로 볼 수도 있다. 

![image.png](attachment:e47434ee-2fd7-49cd-9f4c-b9794c2cbf09:image.png)

전체 20번에서 13번째에 훈련이 조기종료 된 것

열한 번째 에포크에서 가장 낮은 손실을 기록했고  열세 번째 에포크에서 훈련 중지

→ 이렇게 조기종료 기법을 사용하면 안심하고 에포크 횟수 크게 지정해도 괜찮음
